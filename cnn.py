# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ks7JJ1saJmCxbyTsjzO8kvseiGO9hFO1
"""

# cnn_solution.py  â€” Reference solution (StudentCNN implemented: strict order, no BatchNorm)
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Tunable parameters (kept the same as your settings)
BATCH_SIZE = 5000
EPOCHS     = 5000
LR         = 5000
OPTIMIZER  = ""
DROPOUT_P  = 0.3
SEED       = 42

def seed_all(sd=SEED):
    import random, os
    random.seed(sd)
    torch.manual_seed(sd)
    torch.cuda.manual_seed_all(sd)
    os.environ["PYTHONHASHSEED"] = str(sd)

class StudentCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # Block 1: Conv(1->32, k=3,p=1) -> ReLU -> Conv(32->64, k=3,p=1) -> ReLU -> MaxPool(2)

        # Block 2: Conv(64->128, k=3,p=1) -> ReLU -> MaxPool(2)

        # Head: Flatten -> Linear(128*7*7, 256) -> ReLU -> Dropout -> Linear(256, 10)


    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.classifier(x)
        return x

def main():
    seed_all()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Data preprocessing
    transform = transforms.ToTensor()
    train_set = datasets.MNIST("./data", train=True,  download=True, transform=transform)
    test_set  = datasets.MNIST("./data", train=False, download=True, transform=transform)
    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
    test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False)

    # Model & optimizer
    model = StudentCNN().to(device)
    criterion = nn.CrossEntropyLoss()
    if OPTIMIZER.lower() == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)
    else:
        optimizer = optim.Adam(model.parameters(), lr=LR)

    # Training & evaluation
    for epoch in range(1, EPOCHS + 1):
        # Train
        model.train()
        total, correct, loss_sum = 0, 0, 0.0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
            loss_sum += loss.item() * y.size(0)
            pred = logits.argmax(1)
            correct += (pred == y).sum().item()
            total += y.size(0)
        train_loss = loss_sum / total
        train_acc = correct / total

        # Eval
        model.eval()
        total, correct, loss_sum = 0, 0, 0.0
        with torch.no_grad():
            for x, y in test_loader:
                x, y = x.to(device), y.to(device)
                logits = model(x)
                loss_sum += criterion(logits, y).item() * y.size(0)
                pred = logits.argmax(1)
                correct += (pred == y).sum().item()
                total += y.size(0)
        test_loss = loss_sum / total
        test_acc = correct / total

        print(f"Epoch {epoch:02d} | "
              f"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | "
              f"Test Loss {test_loss:.4f} Acc {test_acc*100:.2f}%")

if __name__ == "__main__":
    main()