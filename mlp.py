# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ks7JJ1saJmCxbyTsjzO8kvseiGO9hFO1
"""

# mlp.py  —— Minimal runnable MLP on MNIST
# Students can directly modify the hyperparameters at the top.

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


# ===== Hyperparameters =====
LAYERS      = [8, 4]   # Hidden layer sizes.
BATCH_SIZE  = 8        # Batch size
EPOCHS      = 15        # Number of training epochs
LR          = 0.5      # Learning rate
OPTIMIZER   = "adam"    # Choose "adam" or "sgd"
STUDENT_ID = "907394064"

# =======================================================
SEED        = 42        # Random seed for reproducibility
def seed_all(sd=SEED):
    import random, os
    random.seed(sd)
    torch.manual_seed(sd)
    torch.cuda.manual_seed_all(sd)
    os.environ["PYTHONHASHSEED"] = str(sd)

# Simple MLP model
class MLP(nn.Module):
    def __init__(self, input_size=28*28, hidden_sizes=LAYERS, num_classes=10):
        super().__init__()
        layers = []
        in_dim = input_size
        for h in hidden_sizes:
            layers += [nn.Linear(in_dim, h), nn.ReLU()]
            in_dim = h
        layers += [nn.Linear(in_dim, num_classes)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        # Flatten 28x28 images into a vector
        x = x.view(x.size(0), -1)
        return self.net(x)

def main():
    seed_all()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # —— Data preprocessing (same for CNN, except CNN does not flatten) ——
    transform = transforms.ToTensor()
    train_set = datasets.MNIST("./data", train=True,  download=True, transform=transform)
    test_set  = datasets.MNIST("./data", train=False, download=True, transform=transform)
    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
    test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False)

    # --- task 1 ---
    fig, axes = plt.subplots(2, 5, figsize=(8, 4))
    wanted = [int(d) for d in STUDENT_ID] + [0] # last digit forced to be 0

    # building an index list 'picks' that grabs the matching label for each wanted digit in order
    picks = []
    seq = wanted.copy()
    i = 0
    while seq and i < len(train_set):
        label = train_set[i][1]
        if label == seq[0]:
            picks.append(i)
            seq.pop(0)
        i += 1

    # plot
    for i, idx in enumerate(picks):
        img, lab = train_set[idx]
        ax = axes[i//5, i%5]
        ax.imshow(img.squeeze(), cmap="gray")
        ax.set_title(str(lab))
        ax.axis("off")
        
    plt.tight_layout()
    plt.show()

    # —— Model and optimizer ——
    model = MLP().to(device)
    criterion = nn.CrossEntropyLoss()
    if OPTIMIZER.lower() == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)
    else:
        optimizer = optim.Adam(model.parameters(), lr=LR)

    # —— Training and testing loop ——
    for epoch in range(1, EPOCHS + 1):
        # Training
        model.train()
        total, correct, loss_sum = 0, 0, 0.0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()

            loss_sum += loss.item() * y.size(0)
            pred = logits.argmax(1)
            correct += (pred == y).sum().item()
            total += y.size(0)
        train_loss = loss_sum / total
        train_acc = correct / total

        # Testing
        model.eval()
        total, correct, loss_sum = 0, 0, 0.0
        with torch.no_grad():
            for x, y in test_loader:
                x, y = x.to(device), y.to(device)
                logits = model(x)
                loss_sum += criterion(logits, y).item() * y.size(0)
                pred = logits.argmax(1)
                correct += (pred == y).sum().item()
                total += y.size(0)
        test_loss = loss_sum / total
        test_acc = correct / total

        print(f"Epoch {epoch:02d} | "
              f"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | "
              f"Test Loss {test_loss:.4f} Acc {test_acc*100:.2f}%")

if __name__ == "__main__":
    main()